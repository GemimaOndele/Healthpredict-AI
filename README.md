# HealthPredict AI ‚Äî Maintenance pr√©dictive des √©quipements m√©dicaux

Application Streamlit pour analyser des rapports d‚Äôincidents (OpenFDA & documents) et pr√©dire la criticit√© via IA (TF-IDF/LogReg ou CamemBERT + classif). OCR, traduction EN‚ÜíFR optionnelle, similarit√© historique, export CSV/XLSX, historique SQLite.

## ‚ú® Fonctionnalit√©s
- Chargement dataset (processed/raw), fallback auto.
- Pr√©diction texte libre & documents (PDF, DOCX, images ‚Üí OCR).
- Mots-cl√©s TF-IDF, cas similaires (cosinus).
- Traduction EN‚ÜíFR (Transformers, optionnel).
- Historique des pr√©dictions (SQLite).
- Graphiques de tendance (Altair).

## üöÄ D√©marrage rapide (local)
```bash #terminal git bash
python -m venv .venv && source .env/bin/activate   
python -m venv .henv && source .henv/bin/activate  

# (Windows: .henv\Scripts\activate)
.henv\Scripts\activate

pip install --no-cache-dir -r requirements.txt  #ou
pip install -r requirements.txt
# (optionnel) torch CPU:
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
# (optionnel) spaCy mod√®les:
python -m spacy download fr_core_news_sm && python -m spacy download en_core_web_sm

# Variables (optionnel)
cp .env.example .env

#Installer hugging face(plateforme publique) qui contient la donn√©es utilis√© et les mod√®les d'IA que j'ai entrain√©s ainsi que d'autres mod√®les d'IA disponible pr√© entrain√©s 
pip install huggingface_hub

#installer quelques d√©pendances 
pip install joblib 
pip install scikit-learn
pip install install-scripts
pip install db
pip install scripts
pip install scripts.build_processed_csv
pip install --no-cache-dir python-dotenv
pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
python -m spacy download fr_core_news_sm
python -m spacy download en_core_web_sm
pip install --no-cache-dir joblib scikit-learn huggingface_hub

#lancer le t√©l√©chargement des mod√®les IA entrain√©es publi√© sur hugging face  
python scripts/download_assets.py

#lancer le g√©n√©ration du csv pour entrainer le mod√®le TFIDF par la suite
python scripts/build_processed_csv.py

#lancer l'entrainement du mod√®le TFIDF
python scripts/train_minimal_tfidf.py 

#lancer l'entrainement du mod√®le BERT (camemBERT)
python scripts/train_camembert_baseline.py

# (option) activer CamemBERT pour l'√©val
$env:HP_USE_CAMEMBERT="1"

#lancer l'√©valuation des pr√©dictions 
python notebooks/eval_healthpredict.py


# Dans PowerShell, √† la racine du projet
$env:HP_AUTO_DOWNLOAD="1"
$env:HP_DOWNLOAD_CAMEMBERT="1"   # optionnel
$env:HP_DB="C:\Healthpredict-AI-clean\data\app.db"  #(facultatif, sinon data/app.db par d√©faut)

# Lancer l'application :

#1√®re possibilit√©
streamlit run app/healthpredict_app.py  

#2√®me possibilit√©
  .\start.ps1

## Ouvre le lien pour voir l'application IA
http://localhost:8501

```




## üõ†Ô∏è Maintenance & D√©pannage

### üîÑ R√©entra√Ænement des mod√®les

* **TF-IDF** :

  ```bash
  python scripts/train_minimal_tfidf.py
  ```
* **CamemBERT** (plus lourd, n√©cessite torch/transformers) :

  ```bash
  python scripts/train_camembert_baseline.py
  ```

### üóÇÔ∏è Base SQLite

* Si l‚Äôhistorique des pr√©dictions ne fonctionne pas (`no such table: predictions`), ex√©cuter :

  ```bash
  python -c "import hpdb; hpdb.init_db('data/app.db')"
  ```
* Par d√©faut, la base est cr√©√©e dans `data/app.db`.
* Supprimer ce fichier permet de repartir avec une base vide.

### ‚ö° Probl√®mes fr√©quents

* **Erreur NumPy** (`Numpy is not available`) :
  V√©rifier que vous avez `numpy<2` install√© :

  ```bash
  pip install --force-reinstall "numpy<2,>=1.26"
  ```
* **OCR inactif** :

  * V√©rifier que Tesseract est install√© et accessible.
  * Modifier son chemin dans l‚Äôinterface Streamlit si besoin.

### üöÄ Mise √† jour des assets (mod√®les & donn√©es)

* Si un mod√®le ou dataset manque :

  ```bash
  python scripts/download_assets.py
  ```
* Les mod√®les sont aussi disponibles sur [Hugging Face](https://huggingface.co/?activityType=update-dataset&feedType=user).


# Donn√©es ‚Äî HealthPredict AI

## 1) Sources
- **OpenFDA ‚Äì Device Event** (publique) : incidents de dispositifs m√©dicaux.
  - Export initial via `scripts/01_download_openfda.py` (requ√™tes pagin√©es).
  - Fichier brut : `assets/data/raw/raw_openfda_imaging_reports.csv` (et `.jsonl`).
- **Rapports de test** (usage acad√©mique) : documents hospitaliers anonymis√©s (ex. Joigny) pour valider l‚ÄôOCR et la pr√©diction.
  - Trait√©s localement via l‚Äôonglet **üìé Documents** de l‚Äôapp.

## 2) Processus de pr√©paration
- Normalisation & √©tiquetage dans `scripts/build_processed_csv.py`.
- R√©sultat : `assets/data/processed/medical_imaging_text_labeled.csv` avec colonnes cl√©s :
  - `event_text` (texte), `event_type` (Death/Injury/‚Ä¶ si pr√©sent), `label` (0/1), `date_received` (date si pr√©sente).

## 3) Confidentialit√©
- Les fichiers de test contenant des √©l√©ments r√©els sont **anonymis√©s** et **non versionn√©s** publiquement.
- Ne **push** jamais de donn√©es sensibles sur GitHub. Utiliser `.gitignore` pour tout d√©p√¥t de documents r√©els.

## 4) Versionnement & tra√ßabilit√©
- Les artefacts lourds (datasets / mod√®les) sont publi√©s sur **Hugging Face** pour limiter la taille du repo :
  - Mod√®les : `assets/models/*.joblib`
  - Datasets : `assets/data/raw/*`, `assets/data/processed/*` (si n√©cessaire)
- Variables & chemins dans `config/config.yaml` (voir README pour les commandes).

## 5) Contr√¥les qualit√©
- Lancer `scripts/validate_dataset.py` (voir README) pour v√©rifier :
  - Pr√©sence des colonnes obligatoires
  - Encodage UTF-8
  - Duplicats & vides
  - Statistiques de base


## üì¶ Donn√©es & Gouvernance

- **Sources & API** : voir `docs/api_data_sources.md`.
- **Dictionnaire de donn√©es** : voir `docs/data_dictionary.md`.
- **Tra√ßabilit√© / confidentialit√©** : voir `data/README_data.md`.
- **Pipeline** : voir `docs/architecture_data_pipeline.md`.

### ‚úÖ Contr√¥le qualit√© (dataset)

# 1) Configurer les chemins (si besoin)
#   -> config/config.yaml (paths.processed_csv / paths.raw_csv / ...)

# 2) Valider le CSV "processed"
python scripts/validate_dataset.py
# -> code 0 si OK, sinon d√©tail des erreurs (colonnes manquantes, encodage, etc.)


### üîÅ Pr√©paration / entra√Ænement / √©valuation

# G√©n√©rer le processed CSV (si absent)
python scripts/build_processed_csv.py

# Entra√Æner TF-IDF
python scripts/train_minimal_tfidf.py

# (optionnel) Entra√Æner CamemBERT
python scripts/train_camembert_baseline.py

# √âvaluer et g√©n√©rer figures
python notebooks/eval_healthpredict.py
markdown


# 2) Ce qu‚Äôil faut faire (et seulement √ßa)

1) **Cr√©er les fichiers** ci-dessus aux emplacements indiqu√©s.  
2) **Ajouter** la section ‚Äúüì¶ Donn√©es & Gouvernance‚Äù dans le `README.md`.  
3) **Ex√©cuter** rapidement :
   - `python scripts/build_processed_csv.py` (si besoin),
   - `python scripts/validate_dataset.py` (doit afficher `Validation dataset r√©ussie.`).



‚úÖ √Ä METTRE √Ä JOUR (README.md)

Ajoute une petite section ¬´ API REST ¬ª :

## üîå API REST (FastAPI)

L‚ÄôAPI expose des endpoints pour la pr√©diction (texte & fichier) ‚Äî utile pour int√©gration tierce (applications internes, scripts).

**Lancer l‚ÄôAPI**

# depuis la racine du projet
set HP_API_KEY=changeme  # Windows PowerShell: $env:HP_API_KEY="changeme"
uvicorn api.main:app --host 0.0.0.0 --port 8000


  Endpoints
  
  GET /health ‚Üí statut
  
  GET /version ‚Üí info version & mod√®les
  
  POST /predict_text ‚Üí {"text": "...", "model": "tfidf|camembert", "return_keywords": true}
  
  POST /predict_file ‚Üí upload fichier (texte brut c√¥t√© API ; OCR/parse avanc√© via l‚Äôapp Streamlit)
  
  S√©curit√©
  
  Header requis si HP_API_KEY d√©fini : X-API-Key: <cl√©>
  
  Exemple cURL
  
  curl -X POST http://localhost:8000/predict_text \
    -H "Content-Type: application/json" \
    -H "X-API-Key: changeme" \
    -d "{\"text\":\"radiologie en panne et erreurs\",\"model\":\"tfidf\",\"return_keywords\":true}"


## CI : Int√©gration continue
![CI](https://github.com/<TON_ORG>/<TON_REPO>/actions/workflows/ci.yml/badge.svg)


## CD : üö¢ D√©ploiement Docker

### Lancer localement (sans API s√©par√©e)
```bash
docker build -f Dockerfile.app -t healthpredict-app .
docker run --rm -p 8501:8501 -v %cd%/data:/app/data healthpredict-app
# puis http://localhost:8501

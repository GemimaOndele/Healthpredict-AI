# HealthPredict AI ‚Äî Maintenance pr√©dictive des √©quipements m√©dicaux

Application **Streamlit** pour analyser des rapports d‚Äôincidents (OpenFDA & documents) et **pr√©dire la criticit√©** via IA (TF-IDF/LogReg ou CamemBERT+classif). OCR, traduction EN‚ÜíFR optionnelle, similarit√© historique, export CSV/XLSX, historique SQLite.

## ‚ú® Fonctionnalit√©s
- Chargement dataset (processed/raw), fallback auto.
- Pr√©diction texte libre & documents (PDF, DOCX, images ‚Üí OCR).
- Mots-cl√©s TF-IDF et recherche de cas similaires (cosinus).
- Traduction EN‚ÜíFR (Transformers, optionnel).
- Historique des pr√©dictions (SQLite).
- Graphiques de tendance (Altair)/pr√©vision simple.

---

Linux / macOS

python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt


2) Variables (optionnel)
# Windows
Copy-Item .env.example .env
$env:HP_AUTO_DOWNLOAD="1"
$env:HP_DB="$pwd\data\app.db"

# Linux/macOS
cp .env.example .env
export HP_AUTO_DOWNLOAD=1
export HP_DB="$PWD/data/app.db"

3) T√©l√©chargement des assets & pr√©paration des donn√©es
python scripts/download_assets.py
python scripts/build_processed_csv.py
python scripts/validate_dataset.py   # doit afficher "Validation dataset r√©ussie."

4) (Option) entra√Ænement des mod√®les
python scripts/train_minimal_tfidf.py
# CamemBERT (plus lourd)
python scripts/train_camembert_baseline.py

5) Lancer l‚Äôapplication
streamlit run app/healthpredict_app.py
# ‚Üí http://localhost:8501

üîå API REST (FastAPI)

Endpoints utiles pour int√©grations tierces.

Lancer l‚ÄôAPI

$env:HP_API_KEY="changeme"
uvicorn api.main:app --host 0.0.0.0 --port 8000
# ‚Üí http://localhost:8000/docs


Exemples

# Health
curl http://localhost:8000/health

# Pr√©diction texte (TF-IDF + mots-cl√©s)
curl -X POST http://localhost:8000/predict_text \
  -H "Content-Type: application/json" \
  -H "X-API-Key: changeme" \
  -d '{"text":"scanner error overheating pump","model":"tfidf","return_keywords":true}'

üß™ Tests
# (si besoin) rendre le package 'api' importable
# ‚Üí pytest.ini doit contenir:  [pytest]  pythonpath = .
pytest -q

üõ†Ô∏è D√©pannage rapide

DB non initialis√©e / historique d√©sactiv√©

python -c "import hpdb,os; hpdb.init_db(os.environ.get('HP_DB','data/app.db'))"


NumPy / Torch

Pr√©f. numpy>=1.26,<3

CPU Torch (option):

pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu


OCR

Installer Tesseract puis configurer le chemin dans l‚ÄôUI (onglet Documents).

üìÇ Donn√©es

OpenFDA (Device Event) ‚Üí assets/data/raw/‚Ä¶

Processed CSV ‚Üí assets/data/processed/medical_imaging_text_labeled.csv

√âval (ROC/PR/CM) ‚Üí assets/eval/

Mod√®les ‚Üí assets/models/*.joblib

Les jeux et mod√®les lourds sont h√©berg√©s sur Hugging Face (scripts download_assets.py).

üì¶ CI / Qualit√© (optionnel)

Workflow GitHub Actions : pytest (tests) + ruff (lint).

Docker (plus tard) : Dockerfile.app, Dockerfile.api, docker-compose.yml.

üîí Avertissement

Projet √©ducatif/d√©mo. Ne pas utiliser pour d√©cision m√©dicale sans validation clinique.


---

## Ce que tu as √† faire c√¥t√© repo

1) Remplacer **tel quel** les trois fichiers :
- `api/main.py`
- `requirements.txt`
- `README.md`

2) (Si ce n‚Äôest pas d√©j√† le cas) t‚Äôassurer que **`api/__init__.py`** existe (m√™me vide) pour que `from api.main import app` fonctionne pendant `pytest`.

3) V√©rifier que ton **`pytest.ini`** contient bien :
```ini
[pytest]
pythonpath = .


Relancer rapidement :

# Windows PowerShell
.\.henv\Scripts\Activate.ps1
$env:HP_DB="$pwd\data\app.db"
python -c "import hpdb,os; hpdb.init_db(os.environ['HP_DB'])"
pytest -q
uvicorn api.main:app --host 0.0.0.0 --port 8000
streamlit run app/healthpredict_app.py
## üöÄ Installation rapide

### 1) Cr√©er l‚Äôenvironnement

**Windows (PowerShell)**
```powershell
python -m venv .henv
.\.henv\Scripts\Activate.ps1
pip install -r requirements.txt


## üöÄ D√©marrage rapide (local)
```bash #terminal git bash
python -m venv .venv && source .env/bin/activate   
python -m venv .henv && source .henv/bin/activate  

# (Windows: .henv\Scripts\activate)
.henv\Scripts\activate

pip install --no-cache-dir -r requirements.txt  #ou
pip install -r requirements.txt
# (optionnel) torch CPU:
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
# (optionnel) spaCy mod√®les:
python -m spacy download fr_core_news_sm && python -m spacy download en_core_web_sm

# Variables (optionnel)
cp .env.example .env

#Installer hugging face(plateforme publique) qui contient la donn√©es utilis√© et les mod√®les d'IA que j'ai entrain√©s ainsi que d'autres mod√®les d'IA disponible pr√© entrain√©s 
pip install huggingface_hub

#installer quelques d√©pendances 
pip install joblib 
pip install scikit-learn
pip install install-scripts
pip install db
pip install scripts
pip install scripts.build_processed_csv
pip install --no-cache-dir python-dotenv
pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
python -m spacy download fr_core_news_sm
python -m spacy download en_core_web_sm
pip install --no-cache-dir joblib scikit-learn huggingface_hub

#lancer le t√©l√©chargement des mod√®les IA entrain√©es publi√© sur hugging face  
python scripts/download_assets.py

#lancer le g√©n√©ration du csv pour entrainer le mod√®le TFIDF par la suite
python scripts/build_processed_csv.py

#lancer l'entrainement du mod√®le TFIDF
python scripts/train_minimal_tfidf.py 

#lancer l'entrainement du mod√®le BERT (camemBERT)
python scripts/train_camembert_baseline.py

# (option) activer CamemBERT pour l'√©val
$env:HP_USE_CAMEMBERT="1"

#lancer l'√©valuation des pr√©dictions 
python notebooks/eval_healthpredict.py


# Dans PowerShell, √† la racine du projet
$env:HP_AUTO_DOWNLOAD="1"
$env:HP_DOWNLOAD_CAMEMBERT="1"   # optionnel
$env:HP_DB="C:\Healthpredict-AI-clean\data\app.db"  #(facultatif, sinon data/app.db par d√©faut)

# Lancer l'application :

#1√®re possibilit√©
streamlit run app/healthpredict_app.py  

#2√®me possibilit√©
  .\start.ps1

## Ouvre le lien pour voir l'application IA
http://localhost:8501

```




## üõ†Ô∏è Maintenance & D√©pannage

### üîÑ R√©entra√Ænement des mod√®les

* **TF-IDF** :

  ```bash
  python scripts/train_minimal_tfidf.py
  ```
* **CamemBERT** (plus lourd, n√©cessite torch/transformers) :

  ```bash
  python scripts/train_camembert_baseline.py
  ```

### üóÇÔ∏è Base SQLite

* Si l‚Äôhistorique des pr√©dictions ne fonctionne pas (`no such table: predictions`), ex√©cuter :

  ```bash
  python -c "import hpdb; hpdb.init_db('data/app.db')"
  ```
* Par d√©faut, la base est cr√©√©e dans `data/app.db`.
* Supprimer ce fichier permet de repartir avec une base vide.

### ‚ö° Probl√®mes fr√©quents

* **Erreur NumPy** (`Numpy is not available`) :
  V√©rifier que vous avez `numpy<2` install√© :

  ```bash
  pip install --force-reinstall "numpy<2,>=1.26"
  ```
* **OCR inactif** :

  * V√©rifier que Tesseract est install√© et accessible.
  * Modifier son chemin dans l‚Äôinterface Streamlit si besoin.

### üöÄ Mise √† jour des assets (mod√®les & donn√©es)

* Si un mod√®le ou dataset manque :

  ```bash
  python scripts/download_assets.py
  ```
* Les mod√®les sont aussi disponibles sur [Hugging Face](https://huggingface.co/?activityType=update-dataset&feedType=user).


# Donn√©es ‚Äî HealthPredict AI

## 1) Sources
- **OpenFDA ‚Äì Device Event** (publique) : incidents de dispositifs m√©dicaux.
  - Export initial via `scripts/01_download_openfda.py` (requ√™tes pagin√©es).
  - Fichier brut : `assets/data/raw/raw_openfda_imaging_reports.csv` (et `.jsonl`).
- **Rapports de test** (usage acad√©mique) : documents hospitaliers anonymis√©s (ex. Joigny) pour valider l‚ÄôOCR et la pr√©diction.
  - Trait√©s localement via l‚Äôonglet **üìé Documents** de l‚Äôapp.

## 2) Processus de pr√©paration
- Normalisation & √©tiquetage dans `scripts/build_processed_csv.py`.
- R√©sultat : `assets/data/processed/medical_imaging_text_labeled.csv` avec colonnes cl√©s :
  - `event_text` (texte), `event_type` (Death/Injury/‚Ä¶ si pr√©sent), `label` (0/1), `date_received` (date si pr√©sente).

## 3) Confidentialit√©
- Les fichiers de test contenant des √©l√©ments r√©els sont **anonymis√©s** et **non versionn√©s** publiquement.
- Ne **push** jamais de donn√©es sensibles sur GitHub. Utiliser `.gitignore` pour tout d√©p√¥t de documents r√©els.

## 4) Versionnement & tra√ßabilit√©
- Les artefacts lourds (datasets / mod√®les) sont publi√©s sur **Hugging Face** pour limiter la taille du repo :
  - Mod√®les : `assets/models/*.joblib`
  - Datasets : `assets/data/raw/*`, `assets/data/processed/*` (si n√©cessaire)
- Variables & chemins dans `config/config.yaml` (voir README pour les commandes).

## 5) Contr√¥les qualit√©
- Lancer `scripts/validate_dataset.py` (voir README) pour v√©rifier :
  - Pr√©sence des colonnes obligatoires
  - Encodage UTF-8
  - Duplicats & vides
  - Statistiques de base


## üì¶ Donn√©es & Gouvernance

- **Sources & API** : voir `docs/api_data_sources.md`.
- **Dictionnaire de donn√©es** : voir `docs/data_dictionary.md`.
- **Tra√ßabilit√© / confidentialit√©** : voir `data/README_data.md`.
- **Pipeline** : voir `docs/architecture_data_pipeline.md`.

### ‚úÖ Contr√¥le qualit√© (dataset)

# 1) Configurer les chemins (si besoin)
#   -> config/config.yaml (paths.processed_csv / paths.raw_csv / ...)

# 2) Valider le CSV "processed"
python scripts/validate_dataset.py
# -> code 0 si OK, sinon d√©tail des erreurs (colonnes manquantes, encodage, etc.)


### üîÅ Pr√©paration / entra√Ænement / √©valuation

# G√©n√©rer le processed CSV (si absent)
python scripts/build_processed_csv.py

# Entra√Æner TF-IDF
python scripts/train_minimal_tfidf.py

# (optionnel) Entra√Æner CamemBERT
python scripts/train_camembert_baseline.py

# √âvaluer et g√©n√©rer figures
python notebooks/eval_healthpredict.py
markdown

‚úÖ √Ä METTRE √Ä JOUR (README.md)

# 2) Ce qu‚Äôil faut faire 

1) **Cr√©er les fichiers et ex√©cuter** ci-dessus aux emplacements indiqu√©s.  
   - `python scripts/build_processed_csv.py` (si besoin),
   - `python scripts/validate_dataset.py` (doit afficher `Validation dataset r√©ussie.`).


## üîå API REST (FastAPI)

L‚ÄôAPI expose des endpoints pour la pr√©diction (texte & fichier) ‚Äî utile pour int√©gration tierce (applications internes, scripts).

**Lancer l‚ÄôAPI**

# depuis la racine du projet
set HP_API_KEY=changeme  # Windows PowerShell: $env:HP_API_KEY="changeme"
uvicorn api.main:app --host 0.0.0.0 --port 8000


  Endpoints
  
  GET /health ‚Üí statut
  
  GET /version ‚Üí info version & mod√®les
  
  POST /predict_text ‚Üí {"text": "...", "model": "tfidf|camembert", "return_keywords": true}
  
  POST /predict_file ‚Üí upload fichier (texte brut c√¥t√© API ; OCR/parse avanc√© via l‚Äôapp Streamlit)
  
  S√©curit√©
  
  Header requis si HP_API_KEY d√©fini : X-API-Key: <cl√©>
  
  Exemple cURL
  
  curl -X POST http://localhost:8000/predict_text \
    -H "Content-Type: application/json" \
    -H "X-API-Key: changeme" \
    -d "{\"text\":\"radiologie en panne et erreurs\",\"model\":\"tfidf\",\"return_keywords\":true}"


## CI : Int√©gration continue
![CI](https://github.com/<TON_ORG>/<TON_REPO>/actions/workflows/ci.yml/badge.svg)


## CD : üö¢ D√©ploiement Docker

### Lancer localement (sans API s√©par√©e)
```bash
docker build -f Dockerfile.app -t healthpredict-app .
docker run --rm -p 8501:8501 -v %cd%/data:/app/data healthpredict-app
# puis http://localhost:8501
